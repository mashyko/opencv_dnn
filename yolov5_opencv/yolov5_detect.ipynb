{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/\n",
    "\n",
    "Using PyTorchHub\n",
    "The following script downloads a pre trained model from PyTorchHub and passes an image for inference. By default, yolov5s.pt is downloaded unless the name is changed. The results can be printed to console, saved to ./yolov5/runs/hub, displayed on screen(local), and returned as tensors or pandas data frames. You can also play with various inference attributes. Check out this link for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "# Image\n",
    "img = cv2.imread(PATH_TO_IMAGE)\n",
    "# Inference\n",
    "results = model(imgs, size=640)  # includes NMS\n",
    "# Results\n",
    "results.print()  \n",
    "results.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands are for converting the YOLOv5s model. The notebook contains the code to convert and download rest of the models.\n",
    "https://colab.research.google.com/github/spmallick/learnopencv/blob/master/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python/Convert_PyTorch_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository. \n",
    "!git clone https://github.com/ultralytics/YOLOv5\n",
    "\n",
    "%cd YOLOv5 # Install dependencies.\n",
    "!pip install -r requirements.txt\n",
    "!pip install onnx\n",
    "\n",
    "# Download .pt model.\n",
    "!wget https://github.com/ultralytics/YOLOv5/releases/download/v6.1/YOLOv5s.pt\n",
    "\n",
    "%cd .. # Export to ONNX.\n",
    "!python export.py --weights models/YOLOv5s.pt --include onnx\n",
    "\n",
    "# Download the file.\n",
    "from google.colab import files\n",
    "files.download('models/YOLOv5s.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constants INPUT_WIDTH and INPUT_HEIGHT are for the blob size. The BLOB stands for Binary Large Object. It contains the data in readable raw format. The image has to be converted to a blob so that the network can process it. In our case, it is a 4D array object with the shape (1, 3, 640, 640).\n",
    "\n",
    "SCORE_THRESHOLD: To filter low probability class scores.\n",
    "NMS_THRESHOLD: To remove overlapping bounding boxes.\n",
    "CONFIDENCE_THRESHOLD: Filters low probability detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants.\n",
    "INPUT_WIDTH = 640\n",
    "INPUT_HEIGHT = 640\n",
    "SCORE_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.45\n",
    "CONFIDENCE_THRESHOLD = 0.45\n",
    "\n",
    "# Text parameters.\n",
    "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "FONT_SCALE = 0.7\n",
    "THICKNESS = 1\n",
    "\n",
    "# Colors.\n",
    "BLACK  = (0,0,0)\n",
    "BLUE   = (255,178,50)\n",
    "YELLOW = (0,255,255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw Label\n",
    "The function draw_label annotates the class names anchored to the top left corner of the bounding box. The code is fairly simple. We pass the text string as a label in the argument which is passed to the OpenCV function getTextSize(). It returns the size of the bounding box that the text string would take up. These dimension values are used to draw a black background rectangle on which label is rendered by putText() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_label(im, label, x, y):\n",
    "    \"\"\"Draw text onto image at location.\"\"\"\n",
    "    # Get text size.\n",
    "    text_size = cv2.getTextSize(label, FONT_FACE, FONT_SCALE, THICKNESS)\n",
    "    dim, baseline = text_size[0], text_size[1]\n",
    "    # Use text size to create a BLACK rectangle.\n",
    "    cv2.rectangle(im, (x,y), (x + dim[0], y + dim[1] + baseline), (0,0,0), cv2.FILLED);\n",
    "    # Display text inside the rectangle.\n",
    "    cv2.putText(im, label, (x, y + dim[1]), FONT_FACE, FONT_SCALE, YELLOW, THICKNESS, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING\n",
    "The function pre–process takes the image and the network as arguments. At first, the image is converted to a blob. Then it is set as input to the network. The function getUnconnectedOutLayerNames() provides the names of the output layers. It has features of all the layers, through which the image is forward propagated to acquire the detections. After processing, it returns the detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(input_image, net):\n",
    "      # Create a 4D blob from a frame.\n",
    "      blob = cv2.dnn.blobFromImage(input_image, 1/255,  (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
    "\n",
    "      # Sets the input to the network.\n",
    "      net.setInput(blob)\n",
    "\n",
    "      # Run the forward pass to get output of the output layers.\n",
    "      outputs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POST-PROCESSING\n",
    "In the previous function pre_process, we get the detection results as an object. It needs to be unwrapped for further processing. Before discussing the code any further, let us see the shape of this object and what it contains.\n",
    "Filter Good Detections\n",
    "While unwrapping, we need to be careful with the shape. With OpenCV-Python 4.5.5, the object is a tuple of a 3-D array of size 1x row x column. It should be row x column. Hence, the array is accessed from the zeroth index. This issue is not observed in the case of C++.\n",
    "\n",
    "The returned object is a 2-D array. The output depends on the size of the input. For example, with the default input size 640, we get a 2D-array of size 25200×85 (rows and columns). The rows represent the number of detections. So each time the network runs, it predicts 25200 bounding boxes. Every bounding box has a 1-D array of 85 entries that tells the quality of the detection. This information is enough to filter out the desired detections.\n",
    "\n",
    "\n",
    "The first two places are normalized center coordinates of the detected bounding box. Then comes the normalized width and height. Index 4 has the confidence score that tells the probability of the detection being an object. The following 80 entries tell class scores of 80 objects of the COCO dataset 2017, on which the model has been trained.\n",
    "\n",
    "While unwrapping, we need to be careful with the shape. With OpenCV-Python 4.5.5, the object is a tuple of a 3-D array of size 1x row x column. It should be row x column. Hence, the array is accessed from the zeroth index. This issue is not observed in the case of C++.\n",
    "\n",
    "The network generates output coordinates based on the input size of the blob,  i.e. 640. Therefore, the coordinates should be multiplied by the resizing factors to get the actual output. Following steps are involved in unwrapping the detections.\n",
    "\n",
    "Loop through detections.\n",
    "Filter out good detections.\n",
    "Get the index of the best class score.\n",
    "Discard detections with class scores lower than the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(input_image, outputs):\n",
    "      # Lists to hold respective values while unwrapping.\n",
    "      class_ids = []\n",
    "      confidences = []\n",
    "      boxes = []\n",
    "      # Rows.\n",
    "      rows = outputs[0].shape[1]\n",
    "      image_height, image_width = input_image.shape[:2]\n",
    "      # Resizing factor.\n",
    "      x_factor = image_width / INPUT_WIDTH\n",
    "      y_factor =  image_height / INPUT_HEIGHT\n",
    "      # Iterate through detections.\n",
    "      for r in range(rows):\n",
    "            row = outputs[0][0][r]\n",
    "            confidence = row[4]\n",
    "            # Discard bad detections and continue.\n",
    "            if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                  classes_scores = row[5:]\n",
    "                  # Get the index of max class score.\n",
    "                  class_id = np.argmax(classes_scores)\n",
    "                  #  Continue if the class score is above threshold.\n",
    "                  if (classes_scores[class_id] > SCORE_THRESHOLD):\n",
    "                        confidences.append(confidence)\n",
    "                        class_ids.append(class_id)\n",
    "                        cx, cy, w, h = row[0], row[1], row[2], row[3]\n",
    "                        left = int((cx - w/2) * x_factor)\n",
    "                        top = int((cy - h/2) * y_factor)\n",
    "                        width = int(w * x_factor)\n",
    "                        height = int(h * y_factor)\n",
    "                        box = np.array([left, top, width, height])\n",
    "                        boxes.append(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Overlapping Boxes\n",
    "After filtering good detections, we are left with the desired bounding boxes. However, there can be multiple overlapping bounding boxes, which may look like the following.\n",
    "This is solved by performing Non-Maximum Suppression. The function NMSBoxes() takes a list of boxes, calculates IOU(Intersection Over Union), and decides to keep the boxes depending on the NMS_THRESHOLD. Curious about how it works? Check out our previous article on NMS to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform non maximum suppression to eliminate redundant, overlapping boxes with lower confidences.\n",
    "      indices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "      for i in indices:\n",
    "            box = boxes[i]\n",
    "            left = box[0]\n",
    "            top = box[1]\n",
    "            width = box[2]\n",
    "            height = box[3]             \n",
    "            # Draw bounding box.             \n",
    "            cv2.rectangle(input_image, (left, top), (left + width, top + height), BLUE, 3*THICKNESS)\n",
    "            # Class label.                      \n",
    "            label = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])             \n",
    "            # Draw label.             \n",
    "            draw_label(input_image, label, left, top)\n",
    "      return input_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Function\n",
    "Finally, we load the model. Perform pre-processing and post-processing followed by displaying efficiency information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "      # Load class names.\n",
    "      classesFile = \"coco.names\"\n",
    "      classes = None\n",
    "      with open(classesFile, 'rt') as f:\n",
    "            classes = f.read().rstrip('\\n').split('\\n')\n",
    "      # Load image.\n",
    "      frame = cv2.imread(‘traffic.jpg)\n",
    "      # Give the weight files to the model and load the network using       them.\n",
    "      modelWeights = \"YOLOv5s.onnx\"\n",
    "      net = cv2.dnn.readNet(modelWeights)\n",
    "      # Process image.\n",
    "      detections = pre_process(frame, net)\n",
    "      img = post_process(frame.copy(), detections)\n",
    "      \"\"\"\n",
    "      Put efficiency information. The function getPerfProfile returns       the overall time for inference(t) \n",
    "      and the timings for each of the layers(in layersTimes).\n",
    "      \"\"\"\n",
    "      t, _ = net.getPerfProfile()\n",
    "      label = 'Inference time: %.2f ms' % (t * 1000.0 /  cv2.getTickFrequency())\n",
    "      print(label)\n",
    "      cv2.putText(img, label, (20, 40), FONT_FACE, FONT_SCALE,  (0, 0, 255), THICKNESS, cv2.LINE_AA)\n",
    "      cv2.imshow('Output', img)\n",
    "      cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
